{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Context (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3b0c5735d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 929, 908, 891, 534, 436, 39, 838, 659, 468, 349, 507, 487, 119, 768]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Quijote wc example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"/FileStore/tables/2ndixk251466511975346/quijote.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why last code didn't throw an error?\n",
    "(the file doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/FileStore/tables/2ndixk251466511975346/quijote.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8326092543a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now, let's trigger the file not found error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \"\"\"\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \"\"\"\n\u001b[1;32m--> 999\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/FileStore/tables/2ndixk251466511975346/quijote.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's trigger the file not found error\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"https://raw.githubusercontent.com/raul-arrabales/BigData-Hands-on/master/data/quijote_complete.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: https\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-21669a5a430a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \"\"\"\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \"\"\"\n\u001b[1;32m--> 999\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: https\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the file:\n",
    "\n",
    "* Copy&Paste is disabled<br>\n",
    "* HTTPS as file system is disabled for TextFile<br>\n",
    "* wget not installed (yum install wget)<br>\n",
    "* wget https://raw.githubusercontent.com/raul-arrabales/BigData-Hands-on/master/data/quijote_complete.txt\n",
    "* and finally get the file in the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3100\r\n",
      "drwxrwxrwx.  7 root   root      199 Oct 24 12:59 .\r\n",
      "drwxr-xr-x. 10 jovyan users     207 Jan 20  2017 ..\r\n",
      "drwxr-xr-x.  2 jovyan users      71 Oct 19 14:06 .ipynb_checkpoints\r\n",
      "-rw-r--r--.  1 root   root   522197 Oct 24 12:38 movies.dat\r\n",
      "-rw-r--r--.  1 jovyan users   88789 Oct 24 12:59 nb-raul.ipynb\r\n",
      "drwxrwxrwx.  7 root   root      126 Oct 18 19:09 python\r\n",
      "-rw-r--r--.  1 root   root  2198927 Oct 24 11:37 quijote_complete.txt\r\n",
      "drwxrwxrwx.  5 root   root       61 Jan 30  2017 R\r\n",
      "-rw-r--r--.  1 root   root   353382 Oct 24 12:38 ratings_verysmall.dat\r\n",
      "drwxrwxrwx.  5 root   root       61 Jan 30  2017 scala\r\n",
      "drwxrwxrwx.  5 root   root      174 Sep 12 00:30 spark\r\n",
      "-rw-r--r--.  1 jovyan users     898 Sep 12 00:32 Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"quijote_complete.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nested wordcount:\n",
    "wordcounts = rdd1.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
    "        .flatMap(lambda x: x.split()) \\\n",
    "        .map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda x,y:x+y) \\\n",
    "        .map(lambda x:(x[1],x[0])) \\\n",
    "        .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20592, u'que'), (18193, u'de'), (18139, u'y'), (10358, u'la'), (9852, u'a')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step by step (explain each step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " u're-use it under the terms of the Project Gutenberg License included',\n",
       " u'with this eBook or online at www.gutenberg.net',\n",
       " u'',\n",
       " u'',\n",
       " u'Title: Don Quijote',\n",
       " u'',\n",
       " u'Author: Miguel de Cervantes Saavedra',\n",
       " u'',\n",
       " u'Posting Date: April 27, 2010 [EBook #2000]',\n",
       " u'Release Date: December, 1999',\n",
       " u'']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"quijote_complete.txt\")\n",
    "rdd1.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the project gutenberg ebook of don quijote  by miguel de cervantes saavedra',\n",
       " u'',\n",
       " u'this ebook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever   you may copy it  give it away or',\n",
       " u're use it under the terms of the project gutenberg license included',\n",
       " u'with this ebook or online at www gutenberg net',\n",
       " u'',\n",
       " u'',\n",
       " u'title: don quijote',\n",
       " u'']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map( lambda x: x.replace('.',' ').replace('-',' ').replace(',',' ').lower())\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384846"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.flatMap(lambda x: x.split())\n",
    "rdd3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dijeron:', u'vida', u'prudencia', u'tragedia', u'don', u'que']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.takeSample(False,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 1),\n",
       " (u'project', 1),\n",
       " (u'gutenberg', 1),\n",
       " (u'ebook', 1),\n",
       " (u'of', 1),\n",
       " (u'don', 1),\n",
       " (u'quijote', 1),\n",
       " (u'by', 1),\n",
       " (u'miguel', 1),\n",
       " (u'de', 1),\n",
       " (u'cervantes', 1),\n",
       " (u'saavedra', 1),\n",
       " (u'this', 1),\n",
       " (u'ebook', 1),\n",
       " (u'is', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd3.map(lambda x: (x, 1))\n",
    "rdd4.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28417"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd4.reduceByKey(lambda x,y:x+y)\n",
    "rdd5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u\"junto''\"),\n",
       " (1, u'ledan\\xedas'),\n",
       " (1, u'\\xbfventa'),\n",
       " (2, u'canes'),\n",
       " (1, u'acurruc\\xf3'),\n",
       " (15, u'igual'),\n",
       " (12, u'ciudades'),\n",
       " (1, u't\\xeda'),\n",
       " (1, u'viese;'),\n",
       " (10, u'hermana')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6 = rdd5.map(lambda x:(x[1],x[0]))\n",
    "rdd6.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20592, u'que'),\n",
       " (18193, u'de'),\n",
       " (18139, u'y'),\n",
       " (10358, u'la'),\n",
       " (9852, u'a'),\n",
       " (8202, u'en'),\n",
       " (8199, u'el'),\n",
       " (6199, u'no'),\n",
       " (4744, u'los'),\n",
       " (4691, u'se')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = rdd6.sortByKey(False)\n",
    "rdd7.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Data Set load example (karma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3040\r\n",
      "drwxrwxrwx.  7 root   root      199 Oct 24 13:01 .\r\n",
      "drwxr-xr-x. 10 jovyan users     207 Jan 20  2017 ..\r\n",
      "drwxr-xr-x.  2 jovyan users      71 Oct 19 14:06 .ipynb_checkpoints\r\n",
      "-rw-r--r--.  1 root   root   522197 Oct 24 12:38 movies.dat\r\n",
      "-rw-r--r--.  1 jovyan users   26036 Oct 24 13:01 nb-raul.ipynb\r\n",
      "drwxrwxrwx.  7 root   root      126 Oct 18 19:09 python\r\n",
      "-rw-r--r--.  1 root   root  2198927 Oct 24 11:37 quijote_complete.txt\r\n",
      "drwxrwxrwx.  5 root   root       61 Jan 30  2017 R\r\n",
      "-rw-r--r--.  1 root   root   353382 Oct 24 12:38 ratings_verysmall.dat\r\n",
      "drwxrwxrwx.  5 root   root       61 Jan 30  2017 scala\r\n",
      "drwxrwxrwx.  5 root   root      174 Sep 12 00:30 spark\r\n",
      "-rw-r--r--.  1 jovyan users     898 Sep 12 00:32 Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::Toy Story (1995)::Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "2::Jumanji (1995)::Adventure|Children|Fantasy\r\n",
      "3::Grumpier Old Men (1995)::Comedy|Romance\r\n",
      "4::Waiting to Exhale (1995)::Comedy|Drama|Romance\r\n",
      "5::Father of the Bride Part II (1995)::Comedy\r\n",
      "6::Heat (1995)::Action|Crime|Thriller\r\n",
      "7::Sabrina (1995)::Comedy|Romance\r\n",
      "8::Tom and Huck (1995)::Adventure|Children\r\n",
      "9::Sudden Death (1995)::Action\r\n",
      "10::GoldenEye (1995)::Action|Adventure|Thriller\r\n"
     ]
    }
   ],
   "source": [
    "! head -10 movies.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::122::5::838985046\r\n",
      "1::185::5::838983525\r\n",
      "1::231::5::838983392\r\n",
      "1::292::5::838983421\r\n",
      "1::316::5::838983392\r\n",
      "1::329::5::838983392\r\n",
      "1::355::5::838984474\r\n",
      "1::356::5::838983653\r\n",
      "1::362::5::838984885\r\n",
      "1::364::5::838983707\r\n"
     ]
    }
   ],
   "source": [
    "! head -10 ratings_verysmall.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading CSV files from file into RDDs in cluster memory\n",
    "moviesRDD = sc.textFile('movies.dat')\n",
    "ratingsRDD = sc.textFile('ratings_verysmall.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Movies:\n",
      "[u'1::Toy Story (1995)::Adventure|Animation|Children|Comedy|Fantasy', u'2::Jumanji (1995)::Adventure|Children|Fantasy', u'3::Grumpier Old Men (1995)::Comedy|Romance', u'4::Waiting to Exhale (1995)::Comedy|Drama|Romance']\n",
      "--- Ratings:\n",
      "[u'1::122::5::838985046', u'1::185::5::838983525', u'1::231::5::838983392', u'1::292::5::838983421']\n"
     ]
    }
   ],
   "source": [
    "# See what we've got in the RDDs\n",
    "print('--- Movies:')\n",
    "print(moviesRDD.take(4))\n",
    "print('--- Ratings:')\n",
    "print(ratingsRDD.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1::122::5::838985046',\n",
       " u'1::185::5::838983525',\n",
       " u'1::231::5::838983392',\n",
       " u'1::292::5::838983421',\n",
       " u'1::316::5::838983392',\n",
       " u'1::329::5::838983392']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current data format in the RDD\n",
    "ratingsRDD.take(6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split fields using a map transformation\n",
    "SplittedRatingsRDD = ratingsRDD.map(lambda l : l.split('::'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1', u'122', u'5', u'838985046'],\n",
       " [u'1', u'185', u'5', u'838983525'],\n",
       " [u'1', u'231', u'5', u'838983392'],\n",
       " [u'1', u'292', u'5', u'838983421'],\n",
       " [u'1', u'316', u'5', u'838983392'],\n",
       " [u'1', u'329', u'5', u'838983392']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we've got now: \n",
    "SplittedRatingsRDD.take(6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pairs M/R style for the counting task (Mapper):\n",
    "RatingCountsRDD = SplittedRatingsRDD.map( lambda (uId, mId, r, ts) : (int(uId), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RatingCountsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taking a sample of our partial counts\n",
    "Rsample = RatingCountsRDD.sample(False, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how big the sample is and inspect\n",
    "Rsample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 1), (18, 1), (34, 1), (35, 1), (36, 1), (51, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how big the sample is and inspect\n",
    "Rsample.take(6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate counts by user (Reducer)\n",
    "RatingsByUserRDD = RatingCountsRDD.reduceByKey(lambda r1, r2 : r1 + r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 146),\n",
       " (66, 41),\n",
       " (30, 147),\n",
       " (50, 46),\n",
       " (90, 44),\n",
       " (27, 38),\n",
       " (100, 193),\n",
       " (135, 16),\n",
       " (105, 25),\n",
       " (72, 22)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect:\n",
    "RatingsByUserRDD.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 800),\n",
       " (34, 639),\n",
       " (65, 569),\n",
       " (125, 564),\n",
       " (96, 501),\n",
       " (36, 479),\n",
       " (78, 468),\n",
       " (56, 427),\n",
       " (122, 421),\n",
       " (18, 413)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 users by the number of ratings:\n",
    "RatingsByUserRDD.takeOrdered(10, key=lambda (uId, nr): -nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 800),\n",
       " (34, 639),\n",
       " (65, 569),\n",
       " (125, 564),\n",
       " (96, 501),\n",
       " (36, 479),\n",
       " (78, 468),\n",
       " (56, 427),\n",
       " (122, 421),\n",
       " (18, 413)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nested version of the same using a \"karma\" RDD:\n",
    "karma = (\n",
    "sc.textFile('ratings_verysmall.dat')\n",
    ".map(lambda l : l.split('::'))\n",
    ".map(lambda (uId, mId, r, ts) : (int(uId), 1))\n",
    ".reduceByKey(lambda r1, r2 : r1 + r2)\n",
    ")\n",
    "karma.takeOrdered(10, key=lambda (uId, nr): -nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the Spark SQL context and imports\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext.getOrCreate(sc.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'200'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.getConf(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a simple DataFrame programatically\n",
    "array = [Row(key=\"a\", group=\"vowels\", value=1),\n",
    "         Row(key=\"b\", group=\"consonants\", value=2),\n",
    "         Row(key=\"c\", group=\"consonants\", value=3),\n",
    "         Row(key=\"d\", group=\"consonants\", value=4),\n",
    "         Row(key=\"e\", group=\"vowels\", value=5)]\n",
    "dataframe = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "dataframe.registerTempTable(\"PythonTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.explain of DataFrame[group: string, key: string, value: bigint]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query1 = sqlContext.sql(\"select * from PythonTestTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(group=u'vowels', key=u'a', value=1),\n",
       " Row(group=u'consonants', key=u'b', value=2),\n",
       " Row(group=u'consonants', key=u'c', value=3),\n",
       " Row(group=u'consonants', key=u'd', value=4),\n",
       " Row(group=u'vowels', key=u'e', value=5)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample age data:\n",
    "datosEdad = [('Raul', 22), ('Ana', 32), ('Juan', 46)]\n",
    "df1 = sqlContext.createDataFrame(datosEdad, ['nombre', 'edad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(nombre=u'Ana', edad=32), Row(nombre=u'Juan', edad=46)]\n"
     ]
    }
   ],
   "source": [
    "# Apply filter to age data:\n",
    "filtroEdad = df1.filter(df1.edad >= 30).collect()\n",
    "print filtroEdad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter (isnotnull(edad#28L) && (edad#28L >= 30))\n",
      "+- Scan ExistingRDD[nombre#27,edad#28L]\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.edad >= 30).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre=u'Raul', altura=176),\n",
       " Row(nombre=u'Ana', altura=177),\n",
       " Row(nombre=u'Juan', altura=182)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample height data\n",
    "datosAltura = [('Raul', 176), ('Ana', 177), ('Juan', 182)]\n",
    "df2 = sqlContext.createDataFrame(datosAltura, ['nombre', 'altura'])\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre=u'Ana', edad=32, altura=177),\n",
       " Row(nombre=u'Raul', edad=22, altura=176),\n",
       " Row(nombre=u'Juan', edad=46, altura=182)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join height and age data:\n",
    "df3 = df1.join(df2, 'nombre')\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre=u'Ana', altura=177),\n",
       " Row(nombre=u'Raul', altura=176),\n",
       " Row(nombre=u'Juan', altura=182)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore different queries\n",
    "df4 = df1.join(df2, 'nombre').select(df1.nombre, df2.altura)\n",
    "df4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [nombre#27, altura#57L]\n",
      "+- *SortMergeJoin [nombre#27], [nombre#56], Inner\n",
      "   :- *Sort [nombre#27 ASC], false, 0\n",
      "   :  +- Exchange hashpartitioning(nombre#27, 200)\n",
      "   :     +- *Project [nombre#27]\n",
      "   :        +- *Filter isnotnull(nombre#27)\n",
      "   :           +- Scan ExistingRDD[nombre#27,edad#28L]\n",
      "   +- *Sort [nombre#56 ASC], false, 0\n",
      "      +- Exchange hashpartitioning(nombre#56, 200)\n",
      "         +- *Filter isnotnull(nombre#56)\n",
      "            +- Scan ExistingRDD[nombre#56,altura#57L]\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, 'nombre').select(df1.nombre, df2.altura).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre=u'Ana', altura=177),\n",
       " Row(nombre=u'Raul', altura=176),\n",
       " Row(nombre=u'Juan', altura=182)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = df1.join(df2, 'nombre').select('nombre', 'altura') \n",
    "df5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre=u'Ana'), Row(nombre=u'Raul'), Row(nombre=u'Juan')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = df1.join(df2, 'nombre', 'right_outer').select('nombre')\n",
    "df6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [nombre#56]\n",
      "+- SortMergeJoin [nombre#27], [nombre#56], RightOuter\n",
      "   :- *Sort [nombre#27 ASC], false, 0\n",
      "   :  +- Exchange hashpartitioning(nombre#27, 200)\n",
      "   :     +- *Project [nombre#27]\n",
      "   :        +- Scan ExistingRDD[nombre#27,edad#28L]\n",
      "   +- *Sort [nombre#56 ASC], false, 0\n",
      "      +- Exchange hashpartitioning(nombre#56, 200)\n",
      "         +- *Project [nombre#56]\n",
      "            +- Scan ExistingRDD[nombre#56,altura#57L]\n"
     ]
    }
   ],
   "source": [
    "df6.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Ratings Example (karma)\n",
    "## Using RDD and DataFrames APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading movies dataset as a set of rows\n",
    "movies = (\n",
    "  sc.textFile('movies.dat')\n",
    "  .map(lambda l : l.split('::'))\n",
    "  .map(lambda (mId, t, gs) : Row(mId=int(mId), t=t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading ratings dataset as a set of rows\n",
    "ratings = (\n",
    "  sc.textFile('ratings_verysmall.dat')\n",
    "  .map(lambda l : l.split('::'))\n",
    "  .map(lambda (uId, mId, r, ts) : Row(mId=int(mId), r=float(r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register both as SQL tables\n",
    "sqlContext.createDataFrame(movies).registerTempTable(\"movies\")\n",
    "sqlContext.createDataFrame(ratings).registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(t=u'Transformers (2007)', avgR=5.0),\n",
       " Row(t=u'Steamboat Bill, Jr. (1928)', avgR=5.0),\n",
       " Row(t=u'Lodger, The (1927)', avgR=5.0),\n",
       " Row(t=u'My Darling Clementine (1946)', avgR=5.0),\n",
       " Row(t=u'Junebug (2005)', avgR=5.0),\n",
       " Row(t=u'Navigator, The (1924)', avgR=5.0),\n",
       " Row(t=u'Spiral Staircase, The (1946)', avgR=5.0),\n",
       " Row(t=u'Beauty and the Beast (Belle et la b\\xeate, La) (1946)', avgR=5.0),\n",
       " Row(t=u'Freaks (1932)', avgR=5.0),\n",
       " Row(t=u'Tales from the Crypt Presents: Demon Knight (1995)', avgR=5.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the SQL query\n",
    "avgRs = sqlContext.sql(\"SELECT t, AVG(r) AS avgR \" +\n",
    "\"FROM movies m \" +\n",
    "\"JOIN ratings r ON (m.mId = r.mId) \" +\n",
    "\"GROUP BY t ORDER BY avgR DESC\")\n",
    "\n",
    "# Inspect resutls\n",
    "avgRs.take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "## Counting ratings per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another example: Counting ratings per user.\n",
    "# Load ratings as rows with user id, movie id and rating (float):\n",
    "ratings = (\n",
    "  sc.textFile('ratings_verysmall.dat')\n",
    "  .map(lambda l : l.split('::'))\n",
    "  .map(lambda (uId, mId, r, ts) : Row(uId=int(uId), mId=int(mId), r=float(r))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the table for SQL\n",
    "sqlContext.createDataFrame(ratings).registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL query for counting ratings:\n",
    "UsrRatingsCount = sqlContext.sql(\"SELECT uId, COUNT(*) FROM ratings GROUP BY uId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uId=26, count(1)=32),\n",
       " Row(uId=29, count(1)=42),\n",
       " Row(uId=65, count(1)=569),\n",
       " Row(uId=19, count(1)=233),\n",
       " Row(uId=54, count(1)=49),\n",
       " Row(uId=112, count(1)=403),\n",
       " Row(uId=113, count(1)=37),\n",
       " Row(uId=22, count(1)=47),\n",
       " Row(uId=130, count(1)=56),\n",
       " Row(uId=7, count(1)=109)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect results:\n",
    "UsrRatingsCount.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL query for counting ratings (ordered):\n",
    "UsrRatingsCount = sqlContext.sql(\"SELECT uId, COUNT(*) FROM ratings GROUP BY uId ORDER BY COUNT(*) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uId=8, count(1)=800),\n",
       " Row(uId=34, count(1)=639),\n",
       " Row(uId=65, count(1)=569),\n",
       " Row(uId=125, count(1)=564),\n",
       " Row(uId=96, count(1)=501),\n",
       " Row(uId=36, count(1)=479),\n",
       " Row(uId=78, count(1)=468),\n",
       " Row(uId=56, count(1)=427),\n",
       " Row(uId=122, count(1)=421),\n",
       " Row(uId=18, count(1)=413)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect results:\n",
    "UsrRatingsCount.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL query for counting ratings (ordered and good formatting):\n",
    "UsrRatingsCount = sqlContext.sql(\"SELECT uId, COUNT(*) AS UserCount \" +\n",
    "                                 \"FROM ratings \" +\n",
    "                                 \"GROUP BY uId \" +\n",
    "                                 \"ORDER BY UserCount DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uId=8, UserCount=800),\n",
       " Row(uId=34, UserCount=639),\n",
       " Row(uId=65, UserCount=569),\n",
       " Row(uId=125, UserCount=564),\n",
       " Row(uId=96, UserCount=501),\n",
       " Row(uId=36, UserCount=479),\n",
       " Row(uId=78, UserCount=468),\n",
       " Row(uId=56, UserCount=427),\n",
       " Row(uId=122, UserCount=421),\n",
       " Row(uId=18, UserCount=413)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect results:\n",
    "UsrRatingsCount.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fake Data Generation\n",
    "## Using faker and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading Faker-0.8.6-py2.py3-none-any.whl (666kB)\n",
      "\u001b[K    100% || 675kB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting ipaddress; python_version == \"2.7\" (from faker)\n",
      "  Downloading ipaddress-1.0.18-py2-none-any.whl\n",
      "Requirement already satisfied: six in /opt/conda/envs/python2/lib/python2.7/site-packages (from faker)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/conda/envs/python2/lib/python2.7/site-packages (from faker)\n",
      "Collecting text-unidecode (from faker)\n",
      "  Downloading text_unidecode-1.0-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K    100% || 81kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: ipaddress, text-unidecode, faker\n",
      "Successfully installed faker-0.8.6 ipaddress-1.0.18 text-unidecode-1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker('es_ES')\n",
    "fake.seed(6243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each entry consists of last_name, first_name, ssn, job, and age (at least 1)\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "  name = fake.name().split()\n",
    "  return Row(name[1], name[0], fake.ssn(), fake.job(), abs(2017 - fake.date_time().year) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a helper function to call a function repeatedly\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in xrange(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dataset with 1k entries\n",
    "data = list(repeat(1000, fake_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Llorens', u'Margarita', u'011-35-0561', 'Ceramics designer', 43)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check:\n",
    "data[0][0], data[0][1], data[0][2], data[0][3], data[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check:\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the DataFrame:\n",
    "dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Check DF\n",
    "print dataDF.count()\n",
    "print dataDF.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "dataDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check DataFrame (RDD) partitions\n",
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "   +- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "last_name: string, first_name: string, ssn: string, occupation: string, age: bigint\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "+- Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "   +- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L], functions=[], output=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L])\n",
      "+- Exchange hashpartitioning(last_name#0, first_name#1, ssn#2, occupation#3, age#4L, 200)\n",
      "   +- *HashAggregate(keys=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L], functions=[], output=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L])\n",
      "      +- Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "# See Spark planning: \n",
    "newDF = dataDF.distinct().select('*')\n",
    "newDF.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------------------------------+---+\n",
      "|last_name|first_name|ssn        |occupation                                  |age|\n",
      "+---------+----------+-----------+--------------------------------------------+---+\n",
      "|Llorens  |Margarita |011-35-0561|Ceramics designer                           |43 |\n",
      "|Elorza   |Sonia     |423-41-7894|Field trials officer                        |16 |\n",
      "|de       |Raquel    |161-64-7629|Brewing technologist                        |35 |\n",
      "|Len     |Teresa    |325-79-6419|Geologist, engineering                      |14 |\n",
      "|Gimenez  |Remedios  |206-14-9369|Further education lecturer                  |43 |\n",
      "|Ramon    |Jose      |091-82-8370|Designer, industrial/product                |3  |\n",
      "|Cuenca   |Emilia    |283-17-0799|Best boy                                    |46 |\n",
      "|Ramon    |Jose      |825-45-6694|Nurse, learning disability                  |23 |\n",
      "|Abascal  |Elena     |382-07-1423|Police officer                              |28 |\n",
      "|Barber  |scar     |880-20-1544|Aeronautical engineer                       |40 |\n",
      "|Bonilla  |Jordi     |703-55-3324|Systems analyst                             |4  |\n",
      "|Ro      |Josefina  |795-72-7974|Software engineer                           |45 |\n",
      "|Portillo |Ignacio   |681-98-9500|Lighting technician, broadcasting/film/video|42 |\n",
      "|Lloret   |Carlos    |075-57-8290|Data scientist                              |19 |\n",
      "|Saura    |scar     |339-14-4786|Museum education officer                    |25 |\n",
      "|de       |Marc      |342-63-9736|Outdoor activities/education manager        |41 |\n",
      "|Seco     |Blanca    |762-77-8196|Administrator, Civil Service                |45 |\n",
      "|Cerd    |Pablo     |733-50-4250|Recycling officer                           |38 |\n",
      "|Castaeda|Esther    |171-33-1817|Education officer, museum                   |40 |\n",
      "|del      |Gregorio  |878-72-5753|Maintenance engineer                        |7  |\n",
      "|Alicia   |Montserrat|797-81-0051|Buyer, industrial                           |33 |\n",
      "|del      |Emilia    |213-43-3184|Advertising account planner                 |41 |\n",
      "|Lastra   |Adriana   |590-99-3251|Sports development officer                  |9  |\n",
      "|Luis     |Mario     |662-50-2673|Nurse, mental health                        |46 |\n",
      "|Llanos   |Montserrat|234-74-7943|Leisure centre manager                      |13 |\n",
      "|Alvarado |Trinidad  |696-61-6687|Pensions consultant                         |18 |\n",
      "|Paula    |Elena     |167-71-6965|Geochemist                                  |20 |\n",
      "|Marn    |Martin    |396-82-8651|Games developer                             |29 |\n",
      "|Arteaga  |Mohamed   |586-42-9403|Scientist, water quality                    |6  |\n",
      "|Fiol     |Santiago  |024-10-8844|Call centre manager                         |26 |\n",
      "+---------+----------+-----------+--------------------------------------------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the data\n",
    "dataDF.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+-----------------------------------------+---+\n",
      "|last_name     |first_name|ssn        |occupation                               |age|\n",
      "+--------------+----------+-----------+-----------------------------------------+---+\n",
      "|Ramon         |Jose      |091-82-8370|Designer, industrial/product             |3  |\n",
      "|Bonilla       |Jordi     |703-55-3324|Systems analyst                          |4  |\n",
      "|del           |Gregorio  |878-72-5753|Maintenance engineer                     |7  |\n",
      "|Lastra        |Adriana   |590-99-3251|Sports development officer               |9  |\n",
      "|Arteaga       |Mohamed   |586-42-9403|Scientist, water quality                 |6  |\n",
      "|Julia         |Teresa    |504-08-0876|Chief Strategy Officer                   |6  |\n",
      "|Peir-Caldern|Manuel    |080-56-6352|Electrical engineer                      |5  |\n",
      "|Palomino      |Cesar     |472-06-7982|Production assistant, radio              |1  |\n",
      "|Luis          |Alejandro |413-06-2025|Sound technician, broadcasting/film/video|8  |\n",
      "|Herrera-Polo  |Rodrigo   |550-39-5486|Chemical engineer                        |9  |\n",
      "|Sebastin     |Belen     |005-08-7555|Furniture conservator/restorer           |7  |\n",
      "|Camacho       |Andrea    |803-25-2937|Television production assistant          |6  |\n",
      "|Llano         |Celia     |841-55-5274|Podiatrist                               |3  |\n",
      "|del           |Gloria    |470-16-2507|Pharmacist, community                    |9  |\n",
      "|Riquelme      |Ramon     |232-81-5352|Therapeutic radiographer                 |4  |\n",
      "|Macas-Cadenas|Elisa     |340-98-2870|Writer                                   |5  |\n",
      "|Almansa       |Carolina  |115-47-7146|Accountant, chartered management         |2  |\n",
      "|Sofia         |Rosa      |561-97-2302|Sound technician, broadcasting/film/video|1  |\n",
      "|Jose          |Francisco |069-91-5424|Embryologist, clinical                   |4  |\n",
      "|Tur           |Nieves    |201-65-0577|Publishing copy                          |2  |\n",
      "+--------------+----------+-----------+-----------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying filters\n",
    "filteredDF = dataDF.filter(dataDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "| Palomino|     Cesar|472-06-7982|Production assist...|  1|\n",
      "| Gonzalez|     Elisa|506-70-9419|Education officer...|  1|\n",
      "|   Asenjo|   Nicols|661-02-5739|      Therapist, art|  1|\n",
      "|    Sofia|      Rosa|561-97-2302|Sound technician,...|  1|\n",
      "|     Jos|      Juan|764-53-1805|      Energy manager|  1|\n",
      "|       de|     Celia|137-94-7391|           Economist|  1|\n",
      "|   Madrid|     Pilar|334-03-6613|          Printmaker|  1|\n",
      "|   Manuel|      Jose|503-89-8515|Programmer, appli...|  1|\n",
      "|   Pineda|  Remedios|563-26-4011|   Financial planner|  1|\n",
      "|     Jose| Francisco|282-22-9611|Corporate investm...|  1|\n",
      "|  Ferrero|     Nuria|742-22-4329|        Retail buyer|  1|\n",
      "|     Leal|  Trinidad|856-16-7935|   Ceramics designer|  1|\n",
      "|   Carlos|      Jose|246-56-9392|Multimedia specia...|  1|\n",
      "|     Alex|    Hector|770-07-0636|   Financial adviser|  1|\n",
      "|  Noguera|    Silvia|639-35-4297|Furniture conserv...|  2|\n",
      "|  Almansa|  Carolina|115-47-7146|Accountant, chart...|  2|\n",
      "|      Tur|    Nieves|201-65-0577|     Publishing copy|  2|\n",
      "|  Ignacio|     Toms|709-18-0580|Production assist...|  2|\n",
      "|   Sotelo|    Lorena|424-42-6781|Conservator, muse...|  2|\n",
      "|    Manso|       Luz|371-55-4301|Diagnostic radiog...|  2|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DF API Queries\n",
    "temp = dataDF.orderBy('age')\n",
    "temp.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(last_name=u'Alex', first_name=u'Hector', ssn=u'770-07-0636', occupation=u'Financial adviser', age=1),\n",
       " Row(last_name=u'Leal', first_name=u'Trinidad', ssn=u'856-16-7935', occupation=u'Ceramics designer', age=1),\n",
       " Row(last_name=u'Gonzalez', first_name=u'Elisa', ssn=u'506-70-9419', occupation=u'Education officer, community', age=1),\n",
       " Row(last_name=u'Sofia', first_name=u'Rosa', ssn=u'561-97-2302', occupation=u'Sound technician, broadcasting/film/video', age=1),\n",
       " Row(last_name=u'Pineda', first_name=u'Remedios', ssn=u'563-26-4011', occupation=u'Financial planner', age=1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF API Queries\n",
    "dataDF.orderBy('age').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----+\n",
      "|occupation                             |count|\n",
      "+---------------------------------------+-----+\n",
      "|Engineer, aeronautical                 |2    |\n",
      "|Diplomatic Services operational officer|1    |\n",
      "|Librarian, academic                    |3    |\n",
      "|Catering manager                       |1    |\n",
      "|Patent examiner                        |2    |\n",
      "|Primary school teacher                 |1    |\n",
      "|English as a second language teacher   |2    |\n",
      "|Estate agent                           |4    |\n",
      "|Control and instrumentation engineer   |1    |\n",
      "|Art therapist                          |1    |\n",
      "|Transport planner                      |2    |\n",
      "|Petroleum engineer                     |2    |\n",
      "|Stage manager                          |1    |\n",
      "|Conservator, furniture                 |1    |\n",
      "|Investment banker, operational         |2    |\n",
      "|Adult nurse                            |4    |\n",
      "|Administrator, Civil Service           |2    |\n",
      "|Counsellor                             |2    |\n",
      "|Sports administrator                   |1    |\n",
      "|Product manager                        |2    |\n",
      "+---------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----+\n",
      "|occupation                          |count|\n",
      "+------------------------------------+-----+\n",
      "|Librarian, academic                 |3    |\n",
      "|Engineer, aeronautical              |1    |\n",
      "|Catering manager                    |1    |\n",
      "|Patent examiner                     |1    |\n",
      "|English as a second language teacher|1    |\n",
      "|Estate agent                        |3    |\n",
      "|Control and instrumentation engineer|1    |\n",
      "|Transport planner                   |1    |\n",
      "|Stage manager                       |1    |\n",
      "|Investment banker, operational      |2    |\n",
      "|Adult nurse                         |2    |\n",
      "|Petroleum engineer                  |1    |\n",
      "|Sports administrator                |1    |\n",
      "|Product manager                     |1    |\n",
      "|Chartered public finance accountant |2    |\n",
      "|Personal assistant                  |1    |\n",
      "|Journalist, newspaper               |1    |\n",
      "|Corporate investment banker         |1    |\n",
      "|Theatre director                    |1    |\n",
      "|Child psychotherapist               |1    |\n",
      "+------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.filter(dataDF.age < 18).groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----+\n",
      "|occupation                                 |count|\n",
      "+-------------------------------------------+-----+\n",
      "|Field trials officer                       |8    |\n",
      "|Chief Strategy Officer                     |7    |\n",
      "|Amenity horticulturist                     |7    |\n",
      "|Horticultural therapist                    |6    |\n",
      "|Psychologist, prison and probation services|6    |\n",
      "|Geologist, engineering                     |6    |\n",
      "|Waste management officer                   |6    |\n",
      "|Armed forces technical officer             |5    |\n",
      "|Sales executive                            |5    |\n",
      "|Chief Financial Officer                    |5    |\n",
      "|Lecturer, further education                |5    |\n",
      "|Software engineer                          |5    |\n",
      "|Insurance account manager                  |5    |\n",
      "|Dance movement psychotherapist             |5    |\n",
      "|Hospital pharmacist                        |4    |\n",
      "|Editor, film/video                         |4    |\n",
      "|Holiday representative                     |4    |\n",
      "|Arts development officer                   |4    |\n",
      "|Multimedia specialist                      |4    |\n",
      "|Adult nurse                                |4    |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "occCount = dataDF.filter(dataDF.age < 80).groupBy('occupation').count()\n",
    "occCount.sort(col('count').desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          occupation|count|\n",
      "+--------------------+-----+\n",
      "|Field trials officer|    8|\n",
      "|Chief Strategy Of...|    7|\n",
      "|Amenity horticult...|    7|\n",
      "|Horticultural the...|    6|\n",
      "|Psychologist, pri...|    6|\n",
      "|Geologist, engine...|    6|\n",
      "|Waste management ...|    6|\n",
      "|Armed forces tech...|    5|\n",
      "|     Sales executive|    5|\n",
      "|Chief Financial O...|    5|\n",
      "|Lecturer, further...|    5|\n",
      "|   Software engineer|    5|\n",
      "|Insurance account...|    5|\n",
      "|Dance movement ps...|    5|\n",
      "| Hospital pharmacist|    4|\n",
      "|  Editor, film/video|    4|\n",
      "|Holiday represent...|    4|\n",
      "|Arts development ...|    4|\n",
      "|Multimedia specia...|    4|\n",
      "|         Adult nurse|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "occCount.registerTempTable(\"occTable\")\n",
    "res = sqlContext.sql(\"select * from occTable order by count desc\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [count#265L DESC], true, 0\n",
      "+- Exchange rangepartitioning(count#265L DESC, 200)\n",
      "   +- *HashAggregate(keys=[occupation#3], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(occupation#3, 200)\n",
      "         +- *HashAggregate(keys=[occupation#3], functions=[partial_count(1)])\n",
      "            +- *Project [occupation#3]\n",
      "               +- *Filter (isnotnull(age#4L) && (age#4L < 80))\n",
      "                  +- Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from occTable order by count desc\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other way to register the DataFrame as table for SQL usage:\n",
    "sqlContext.registerDataFrameAsTable(dataDF, 'people') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+--------------------+---+\n",
      "|   last_name|  first_name|        ssn|          occupation|age|\n",
      "+------------+------------+-----------+--------------------+---+\n",
      "|       Plaza|      Esther|461-31-3752|  Materials engineer| 47|\n",
      "|       Arnau|       Julio|536-74-3134|       Oceanographer| 47|\n",
      "|     Almazn|Purificacin|502-06-6577|Research officer,...| 48|\n",
      "|          de|      Julian|106-41-3492|Psychologist, cli...| 47|\n",
      "|        Sala|      Mireia|295-75-6933|Careers informati...| 48|\n",
      "|    Mariscal|      Teresa|584-05-1203|Sales promotion a...| 48|\n",
      "|      Zabala|     Ricardo|631-06-5739|Presenter, broadc...| 47|\n",
      "|          de|   Magdalena|145-80-4796|Interior and spat...| 48|\n",
      "|       Lucas|      Marcos|654-07-0812|       Warden/ranger| 48|\n",
      "|          de|      Aurora|274-96-1900|Senior tax profes...| 48|\n",
      "|      Carlos|        Marc|112-07-9242|Newspaper journalist| 47|\n",
      "|        Cobo|     Antonio|622-46-4802|   Film/video editor| 47|\n",
      "|   Pol-Segu|      Daniel|602-38-9848|      Engineer, site| 48|\n",
      "|    Luz-Sanz|     Eduardo|198-66-5856|Trade union resea...| 47|\n",
      "|Ribera-Urea|        Sara|259-68-4946|   Financial manager| 47|\n",
      "|        Puga|  Inmaculada|347-17-9980| Chemist, analytical| 48|\n",
      "|       Maria|        Jose|846-43-9136|Television floor ...| 48|\n",
      "|       Reyes|      Adrin|153-51-9891|Teacher, English ...| 47|\n",
      "|       Soler|     Beatriz|326-98-6283|Clinical psycholo...| 47|\n",
      "| Milla-Matas|        Ivan|308-50-1519|        Video editor| 47|\n",
      "+------------+------------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from people where people.age > 46\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|number|          occupation|\n",
      "+------+--------------------+\n",
      "|     1|Engineer, aeronau...|\n",
      "|     1|Diplomatic Servic...|\n",
      "|     1|Primary school te...|\n",
      "|     1|     Patent examiner|\n",
      "|     1|English as a seco...|\n",
      "|     1|        Estate agent|\n",
      "|     1|       Art therapist|\n",
      "|     1|   Transport planner|\n",
      "|     1|  Petroleum engineer|\n",
      "|     1|Conservator, furn...|\n",
      "|     2|         Adult nurse|\n",
      "|     2|Administrator, Ci...|\n",
      "|     2|          Counsellor|\n",
      "|     1|     Product manager|\n",
      "|     1|Chartered public ...|\n",
      "|     1|Pharmacist, hospital|\n",
      "|     2|          Contractor|\n",
      "|     2|   Financial manager|\n",
      "|     1|      Police officer|\n",
      "|     1|      Hydrogeologist|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(ssn) as number, occupation from people where age > 18 group by occupation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Examples\n",
    "\n",
    "## K-Means Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 jovyan users 72 Oct 25 13:30 kmeans_data.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Our data set \n",
    "! ls -la kmeans*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMeans Spark implementation\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RDD Load data set\n",
    "dataset_RDD = sc.textFile(\"kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.0 0.0 0.0',\n",
       " u'0.1 0.1 0.1',\n",
       " u'0.2 0.2 0.2',\n",
       " u'9.0 9.0 9.0',\n",
       " u'9.1 9.1 9.1',\n",
       " u'9.2 9.2 9.2']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame Load data set\n",
    "dataset_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "  .option(\"header\", \"false\").option(\"delimiter\",\" \").option(\"inferschema\", \"true\") \\\n",
    "  .load(\"kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|_c0|_c1|_c2|\n",
      "+---+---+---+\n",
      "|0.0|0.0|0.0|\n",
      "|0.1|0.1|0.1|\n",
      "|0.2|0.2|0.2|\n",
      "|9.0|9.0|9.0|\n",
      "|9.1|9.1|9.1|\n",
      "|9.2|9.2|9.2|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for training \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, _c2: double, features: vector]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"_c0\",\"_c1\",\"_c2\"], outputCol=\"features\") \n",
    "assembler.transform(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the KMeans model\n",
    "kmeans_estimator = KMeans().setFeaturesCol(\"features\").setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline stages definition\n",
    "pipeline = Pipeline(stages=[assembler, kmeans_estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline training\n",
    "model = pipeline.fit(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the results: \n",
    "results = model.transform(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------+----------+\n",
      "|_c0|_c1|_c2|     features|prediction|\n",
      "+---+---+---+-------------+----------+\n",
      "|0.0|0.0|0.0|    (3,[],[])|         1|\n",
      "|0.1|0.1|0.1|[0.1,0.1,0.1]|         1|\n",
      "|0.2|0.2|0.2|[0.2,0.2,0.2]|         1|\n",
      "|9.0|9.0|9.0|[9.0,9.0,9.0]|         0|\n",
      "|9.1|9.1|9.1|[9.1,9.1,9.1]|         0|\n",
      "|9.2|9.2|9.2|[9.2,9.2,9.2]|         0|\n",
      "+---+---+---+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check results:\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Examples\n",
    "\n",
    "## Give Me Credit Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 jovyan users 7564965 Oct 25 14:26 cs-training.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la cs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_csv = sc.textFile(\"cs-training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'50368,0,0.128888174,60,0,0.017980895,5338,3,1,0,0,0',\n",
       " u'32155,0,0.09860954,48,0,0.26446281,3750,5,0,1,0,0',\n",
       " u'75066,0,0.661458537,57,0,2305,NA,7,0,1,0,3',\n",
       " u'136067,0,0.93299298,44,0,0.478718406,10642,12,0,2,0,0',\n",
       " u'24598,0,0.980747349,28,0,0.217005738,5750,16,0,0,1,0',\n",
       " u'49274,0,0.021256536,49,0,0.307874144,12267,7,0,3,1,2',\n",
       " u'23474,0,0.00373585,69,0,1418,NA,12,0,2,0,0',\n",
       " u'16071,0,0.738260955,34,0,0.151169424,1752,2,0,0,0,0',\n",
       " u'123478,0,0.13843795,38,0,1364,NA,12,0,1,0,0',\n",
       " u'9810,0,0.370114353,51,0,0.394998015,15113,9,0,2,0,3']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_csv.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150001"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u',SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get header with field names\n",
    "header = training_set_csv.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructField' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-d60e598886d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fields in the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfield_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StructField' is not defined"
     ]
    }
   ],
   "source": [
    "# Fields in the training set\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in header.split('\\t')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
